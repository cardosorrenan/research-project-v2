{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77aa050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ad1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(combined_paths, qtd_segments_by_rec, db_alias):\n",
    "    for combined_path in combined_paths:\n",
    "        _, _, db_alias, record_filename = combined_path[0].split(\"/\")\n",
    "        index, record_id, _, _ = record_filename.split(\"_\")\n",
    "        result_filename = f\"./{db_alias}/{index}_{record_id}_result.npy\"\n",
    "\n",
    "        if os.path.exists(result_filename):\n",
    "            continue\n",
    "\n",
    "        rri_segment_path, recording_path = combined_path\n",
    "\n",
    "        rri_segments = np.load(rri_segment_path, allow_pickle=True)\n",
    "        recording_segments = np.load(recording_path, allow_pickle=True)\n",
    "\n",
    "        print(qtd_segments_by_rec, len(rri_segments))\n",
    "\n",
    "        if len(rri_segments) < qtd_segments_by_rec:\n",
    "            continue\n",
    "\n",
    "        index_list = np.random.choice(\n",
    "            len(rri_segments),\n",
    "            qtd_segments_by_rec,\n",
    "            replace=False,\n",
    "        )\n",
    "\n",
    "        rri_segments_samples = rri_segments[index_list]\n",
    "\n",
    "        recording_segments_samples = recording_segments[index_list]\n",
    "\n",
    "        recording_result = np.empty((0, 57))\n",
    "\n",
    "        for segment in tqdm(zip(rri_segments_samples, recording_segments_samples)):\n",
    "            rri_segment, recording = segment\n",
    "\n",
    "            rri_histogram, _ = np.histogram(a=rri_segment, range=(0, 2500), bins=50)\n",
    "\n",
    "            if rri_histogram.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            btd = get_btd(recording)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            combine = np.hstack((rri_histogram, np.array(btd)))\n",
    "\n",
    "            recording_result = np.vstack((recording_result, combine))\n",
    "\n",
    "        np.save(\n",
    "            file=result_filename,\n",
    "            arr=recording_result,\n",
    "        )\n",
    "\n",
    "\n",
    "def get_combined_path(data_path):\n",
    "    data_filenames = os.listdir(data_path)\n",
    "\n",
    "    rri_segment_filenames = list(\n",
    "        filter(lambda item: \"rri_segment\" in item, data_filenames)\n",
    "    )\n",
    "    rri_segment_paths = [f\"{data_path}{filename}\" for filename in rri_segment_filenames]\n",
    "\n",
    "    recording_segment_filenames = list(\n",
    "        filter(lambda item: \"recording_segment\" in item, data_filenames)\n",
    "    )\n",
    "    recording_segment_paths = [\n",
    "        f\"{data_path}{filename}\" for filename in recording_segment_filenames\n",
    "    ]\n",
    "\n",
    "    combine_paths = [\n",
    "        combined_paths\n",
    "        for combined_paths in zip(\n",
    "            sorted(rri_segment_paths), sorted(recording_segment_paths)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return combine_paths\n",
    "\n",
    "\n",
    "def generate_chunks(combined_paths, alias_db):\n",
    "    for combined_path in combined_paths:\n",
    "        _, _, _, record_filename = combined_path[0].split(\"/\")\n",
    "        _, record_id, _, _ = record_filename.split(\"_\")\n",
    "\n",
    "        rri_segment_path, recording_path = combined_path\n",
    "\n",
    "        rri_segments = np.load(rri_segment_path, allow_pickle=True)\n",
    "\n",
    "        recording_segments = np.load(recording_path, allow_pickle=True)\n",
    "\n",
    "        indexes = np.arange(len(rri_segments))\n",
    "        np.random.shuffle(indexes)\n",
    "\n",
    "        shuffled_rri = rri_segments[indexes]\n",
    "        shuffled_recordings = recording_segments[indexes]\n",
    "\n",
    "        chunk_size = 200\n",
    "\n",
    "        qtd_segments = rri_segments.shape[0]\n",
    "\n",
    "        for i in range(0, qtd_segments, chunk_size):\n",
    "            start_index_chunk = i\n",
    "            end_index_chunk = i + chunk_size\n",
    "\n",
    "            if end_index_chunk >= qtd_segments:\n",
    "                end_index_chunk = qtd_segments - 1\n",
    "\n",
    "            result_filename = f\"./{alias_db}/{record_id}_{start_index_chunk}_{end_index_chunk}_result.npy\"\n",
    "\n",
    "            if os.path.exists(result_filename):\n",
    "                print(\"exists\")\n",
    "                continue\n",
    "\n",
    "            rri_segment_chunk = shuffled_rri[start_index_chunk:end_index_chunk]\n",
    "            recording_chunk = shuffled_recordings[start_index_chunk:end_index_chunk]\n",
    "            zipped_chunk = list(zip(rri_segment_chunk, recording_chunk))\n",
    "\n",
    "            np.save(file=result_filename, arr=zipped_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5f33a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renan/√Årea de Trabalho/research-project-2/venv/lib/python3.8/site-packages/numpy/lib/npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "alias_db = \"NSRDB\"\n",
    "nsrdb_data_path = f\"../extract_data/{alias_db}/\"\n",
    "nsrdb_combine_paths = get_combined_path(nsrdb_data_path)\n",
    "\n",
    "nsrdb_feature_path = f\"./{alias_db}\"\n",
    "if not os.path.exists(nsrdb_feature_path):\n",
    "    os.makedirs(nsrdb_feature_path)\n",
    "\n",
    "nsrdb_combine_paths_shuffled = random.sample(\n",
    "    nsrdb_combine_paths, len(nsrdb_combine_paths)\n",
    ")\n",
    "\n",
    "nsrdb_combine_paths_shuffled_sampled = nsrdb_combine_paths_shuffled\n",
    "\n",
    "generate_chunks(nsrdb_combine_paths_shuffled_sampled, alias_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd9196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_db = \"AFDB\"\n",
    "afdb_data_path = f\"../extract_data/{alias_db}/\"\n",
    "afdb_combine_paths = get_combined_path(afdb_data_path)\n",
    "\n",
    "afdb_feature_path = f\"./{alias_db}\"\n",
    "if not os.path.exists(afdb_feature_path):\n",
    "    os.makedirs(afdb_feature_path)\n",
    "\n",
    "afdb_combine_paths_shuffled = random.sample(afdb_combine_paths, len(afdb_combine_paths))\n",
    "\n",
    "afdb_combine_paths_shuffled_sampled = afdb_combine_paths_shuffled\n",
    "\n",
    "# generate_chunks(afdb_combine_paths_shuffled_sampled, alias_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e352fde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../extract_data/AFDB/4_04746_rri_segment.npy',\n",
       "  '../extract_data/AFDB/4_04746_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/7_05091_rri_segment.npy',\n",
       "  '../extract_data/AFDB/7_05091_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/18_08219_rri_segment.npy',\n",
       "  '../extract_data/AFDB/18_08219_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/3_04126_rri_segment.npy',\n",
       "  '../extract_data/AFDB/3_04126_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/21_08434_rri_segment.npy',\n",
       "  '../extract_data/AFDB/21_08434_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/17_08215_rri_segment.npy',\n",
       "  '../extract_data/AFDB/17_08215_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/10_06426_rri_segment.npy',\n",
       "  '../extract_data/AFDB/10_06426_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/12_06995_rri_segment.npy',\n",
       "  '../extract_data/AFDB/12_06995_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/20_08405_rri_segment.npy',\n",
       "  '../extract_data/AFDB/20_08405_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/9_05261_rri_segment.npy',\n",
       "  '../extract_data/AFDB/9_05261_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/13_07162_rri_segment.npy',\n",
       "  '../extract_data/AFDB/13_07162_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/1_04043_rri_segment.npy',\n",
       "  '../extract_data/AFDB/1_04043_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/5_04908_rri_segment.npy',\n",
       "  '../extract_data/AFDB/5_04908_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/22_08455_rri_segment.npy',\n",
       "  '../extract_data/AFDB/22_08455_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/8_05121_rri_segment.npy',\n",
       "  '../extract_data/AFDB/8_05121_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/2_04048_rri_segment.npy',\n",
       "  '../extract_data/AFDB/2_04048_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/16_07910_rri_segment.npy',\n",
       "  '../extract_data/AFDB/16_07910_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/11_06453_rri_segment.npy',\n",
       "  '../extract_data/AFDB/11_06453_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/19_08378_rri_segment.npy',\n",
       "  '../extract_data/AFDB/19_08378_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/15_07879_rri_segment.npy',\n",
       "  '../extract_data/AFDB/15_07879_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/0_04015_rri_segment.npy',\n",
       "  '../extract_data/AFDB/0_04015_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/6_04936_rri_segment.npy',\n",
       "  '../extract_data/AFDB/6_04936_recording_segment.npy'),\n",
       " ('../extract_data/AFDB/14_07859_rri_segment.npy',\n",
       "  '../extract_data/AFDB/14_07859_recording_segment.npy')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afdb_combine_paths_shuffled_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42e23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_db = \"LTAFDB\"\n",
    "ltafdb_data_path = f\"../extract_data/{alias_db}/\"\n",
    "ltafdb_combine_paths = get_combined_path(ltafdb_data_path)\n",
    "\n",
    "ltafdb_feature_path = f\"./{alias_db}\"\n",
    "if not os.path.exists(ltafdb_feature_path):\n",
    "    os.makedirs(ltafdb_feature_path)\n",
    "\n",
    "ltafdb_combine_paths_shuffled = random.sample(\n",
    "    ltafdb_combine_paths, len(ltafdb_combine_paths)\n",
    ")\n",
    "\n",
    "ltafdb_combine_paths_shuffled_sampled = ltafdb_combine_paths_shuffled\n",
    "\n",
    "generate_chunks(ltafdb_combine_paths_shuffled_sampled, alias_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74ef09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renan/√Årea de Trabalho/research-project-2/venv/lib/python3.8/site-packages/numpy/lib/npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "alias_db = \"LTAFDB_N\"\n",
    "ltafdb_data_path = f\"../extract_data/{alias_db}/\"\n",
    "ltafdb_combine_paths = get_combined_path(ltafdb_data_path)\n",
    "\n",
    "ltafdb_feature_path = f\"./{alias_db}\"\n",
    "if not os.path.exists(ltafdb_feature_path):\n",
    "    os.makedirs(ltafdb_feature_path)\n",
    "\n",
    "ltafdb_combine_paths_shuffled = random.sample(\n",
    "    ltafdb_combine_paths, len(ltafdb_combine_paths)\n",
    ")\n",
    "\n",
    "ltafdb_combine_paths_shuffled_sampled = ltafdb_combine_paths_shuffled\n",
    "\n",
    "generate_chunks(ltafdb_combine_paths_shuffled_sampled, alias_db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
